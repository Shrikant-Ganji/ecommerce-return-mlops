# ğŸ›ï¸ Ecommerce Return Prediction â€” End-to-End MLOps Project

This repository demonstrates a complete **end-to-end MLOps pipeline** using:

* **Prefect 2.0** for orchestration and scheduled deployments
* **MLflow** for experiment tracking and model registry
* **Evidently AI** for monitoring data drift
* **Scikit-learn + Joblib** for modeling
* **GitHub Codespaces + Render** for development and deployment
* **GitHub Actions** for CI/CD automation
* **Terraform** for Infrastructure as Code (planned)

---

## ğŸ¯ Project Context & Objective

### ğŸ“¦ Context

Product returns are a significant challenge in the e-commerce industry. They not only affect revenue but also disrupt supply chains and impact customer satisfaction. Companies like Amazon and Flipkart gather extensive data related to orders, customers, payments, shipping, and product reviews. Mining these datasets can reveal patterns that help predict whether a product is likely to be returned.

Understanding the likelihood of a product return **before it happens** allows e-commerce platforms to:

* Reduce logistical costs and losses
* Identify potentially problematic listings or sellers
* Improve customer service and satisfaction
* Implement proactive customer communication or incentives

### ğŸ¯ Objective

The objective of this project is to build a machine learning model to **predict whether a product will be returned** based on historical data from an e-commerce platform and then implement a complete **MLOps workflow** around this model. This includes:

* Loading and preprocessing historical customer/order/product data
* Training and tuning a machine learning model (Random Forest)
* Tracking all experiments and model versions using MLflow
* Orchestrating the pipeline using Prefect 2.0 for scheduled runs
* Containerizing and optionally deploying a FastAPI service
* Monitoring data drift using Evidently AI
* Automating pipeline via GitHub Actions CI/CD
* Preparing infrastructure as code using Terraform for portability

This project simulates a production-grade workflow for real-world deployment and lifecycle management of ML solutions.

---

## ğŸ§° Tech Stack Overview

| Component              | Tool / Platform            |
| ---------------------- | -------------------------- |
| Cloud/Dev Environment  | GitHub Codespaces + Render |
| Experiment Tracking    | MLflow                     |
| Workflow Orchestration | Prefect 2.0                |
| Monitoring             | Evidently AI               |
| CI/CD                  | GitHub Actions             |
| Infrastructure as Code | Terraform (Render or GCP)  |

---

## ğŸ“‚ Folder Structure & Module Overview

```bash
â”œâ”€â”€ data/                        # Raw, processed, and prediction data
â”œâ”€â”€ deployment/                 # FastAPI app and Render/K8s deployment config
â”œâ”€â”€ monitoring/                 # Drift detection scripts & HTML reports
â”œâ”€â”€ notebooks/                  # EDA, feature engineering, and training notebooks
â”œâ”€â”€ pipelines/                  # Prefect pipeline script
â”œâ”€â”€ prefect/                    # Prefect CLI configs, monitoring runner
â”œâ”€â”€ src/                        # Source code for training and prediction
â”œâ”€â”€ models/                     # Trained ML model (joblib format)
â”œâ”€â”€ mlruns/, mlflow.db          # MLflow tracking artifacts and metadata
â”œâ”€â”€ .github/workflows/ci.yml    # GitHub Actions pipeline for testing
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ prefect.yaml                # Prefect deployment CLI config
â”œâ”€â”€ Dockerfile, Makefile        # Docker & command automation
â”œâ”€â”€ run_deployment.py           # Script for triggering Prefect flow
â”œâ”€â”€ test_predict.py             # Test script for model inference
â”œâ”€â”€ terraform/                  # Infrastructure as Code (Render/GCP)
â””â”€â”€ README.md                   # Project documentation
```

---

## ğŸ“¦ Module Details

### ğŸ—ƒï¸ `data/`

* `raw/`: Contains original Olist e-commerce dataset CSVs.
* `processed/`: Preprocessed Parquet files: `train`, `test`, `X`, `y` splits.
* `predictions.csv`: Output file from inference pipeline.

### ğŸ§  `src/`

* `data_prep.py`: Loads raw data, handles missing values, encodes, and splits.
* `train.py`: Trains a Random Forest model, tracks with MLflow.
* `predict.py`: Generates predictions using the trained model.
* `utils.py`: Shared helper functions.

### ğŸ““ `notebooks/`

* `eda.ipynb`: Explore distributions, missing values, correlations.
* `feature_engineering.ipynb`: Encoding, scaling, and transformation logic.
* `model_training.ipynb`: Model experimentation + MLflow logging walkthrough.

### ğŸ§ª `models/`

* `return_model.joblib`: Final serialized version of the trained model.

### ğŸ“ `mlruns/`, `mlflow.db`

* MLflow tracking backend for:

  * Parameters
  * Metrics (Accuracy, F1-score)
  * Artifacts (model.pkl, conda.yaml, envs)

---

## âš™ï¸ Prefect 2.0 Orchestration

### ğŸ§µ `pipelines/prefect_flow.py`

Defines `@flow` called `full_pipeline()`:

* Loads â†’ preprocesses â†’ trains â†’ evaluates â†’ predicts â†’ monitors drift

### ğŸ§¾ `prefect.yaml`

* CLI-generated deployment config
* Allows you to run via:

```bash
prefect deploy -n "Ecommerce Return Pipeline"
```

### ğŸ§ª `prefect/run_monitoring_flow.py`

* Triggers monitoring task separately via CLI or script

### ğŸ§¾ `run_deployment.py`

* On-demand triggering of full Prefect pipeline

---

## ğŸ›°ï¸ Deployment Modules

### âš™ï¸ `deployment/app.py`

* Optional FastAPI app for serving predictions via `/predict`

### ğŸ§¾ `deployment/service.yaml`

* Kubernetes/Render-compatible YAML deployment descriptor

---

## ğŸ“Š Monitoring with Evidently

### ğŸ§­ `monitoring/evidently_monitor.py`

* Generates drift report by comparing reference vs current dataset
* Runs as Prefect task or cronjob

### ğŸ“Š `monitoring/evidently_report.html`

* Output HTML report from Evidently with visual metrics

---

## âœ… Tests

### ğŸ” `test_predict.py`

* Verifies that the saved model loads and makes predictions
* Ensures CI/CD test automation

---

## ğŸ Setup & Usage

### ğŸ”§ Environment Setup

```bash
conda create -n mlops311 python=3.11 -y
conda activate mlops311
pip install -r requirements.txt
```

### ğŸš¦ Start Prefect Server & Worker

```bash
prefect server start
prefect worker start -q default
```

### ğŸš€ Deploy the Pipeline

```bash
prefect deploy pipelines/prefect_flow.py:full_pipeline \
  -n "Ecommerce Return Pipeline" \
  -q "default" \
  -p "default-agent-pool"
```

### ğŸ” Trigger Manual Run

```bash
prefect deployment run 'ecommerce-return-mlops/Ecommerce Return Pipeline'
```

---

## ğŸ“ˆ Output & Artifacts

* ğŸ”¹ Drift Report: `monitoring/evidently_report.html`
* ğŸ”¹ Tracked Experiments: MLflow UI under `mlruns/`
* ğŸ”¹ Serialized Model: `models/return_model.joblib`
* ğŸ”¹ Scheduled Deployment: `prefect.yaml`

---

## ğŸ§  Best Practices Implemented

* âœ… GitHub Codespaces-friendly environment
* âœ… MLflow Tracking + Model Registry
* âœ… Prefect Flow Scheduling and Deployment
* âœ… Lightweight Containerized Deployment (optional)
* âœ… Drift Monitoring with Evidently
* âœ… CI/CD Pipeline with GitHub Actions
* âœ… Infrastructure-as-Code setup scaffold with Terraform

---

## ğŸ Future Enhancements

* ğŸ” Automated retraining and model versioning
* ğŸ§ª Unit tests for each module
* ğŸ“£ Slack/email alerts for drift detection
* â˜ï¸ Remote artifact logging (S3/GCS/Weights & Biases)
* ğŸ³ Publish Docker container on Render or GCP

---

## ğŸ‘¨â€ğŸ’» Author

**Shrikant Ganji**
ğŸš€ MLOps Zoomcamp | 2025 Edition
ğŸ”— [GitHub](https://github.com/Shrikant-Ganji)
